4
0
0
2
 
n
a
J
 
7
2
 
 
]
h
p
-
n
e
g
.
s
c
i
s
y
h
p
[
 
 
5
v
0
2
0
1
0
0
0
/
s
c
i
s
y
h
p
:
v
i
X
r
a

Why Occam’s Razor

Russell K. Standish

School of Mathematics

University of New South Wales

Sydney, 2052, Australia

R.Standish@unsw.edu.au, http://parallel.hpc.unsw.edu.au/rks

September 2, 2013

Ensemble theories have received a lot of interest recently as a means

of explaining a lot of the detailed complexity observed in reality by a

vastly simpler description “every possibility exists” and a selection princi-

ple (Anthropic Principle) “we only observe that which is consistent with

our existence”. In this paper I show why, in an ensemble theory of the

universe, we should be inhabiting one of the elements of that ensemble

with least information content that satisﬁes the anthropic principle. This

explains the eﬀectiveness of aesthetic principles such as Occam’s razor in

predicting usefulness of scientiﬁc theories. I also show, with a couple of

reasonable assumptions about the phenomenon of consciousness, the lin-

1

ear structure of quantum mechanics can be derived.

Key words: Occam’s razor, anthropic principle, ensemble theories, multi-

verse, failure of induction, foundation of quantum mechanics

1

INTRODUCTION

Wigner(1) once remarked on “the unreasonable eﬀectiveness of mathematics”,

encapsulating in one phrase the mystery of why the scientiﬁc enterprise is so

successful. There is an aesthetic principle at large, whereby scientiﬁc theories

are chosen according to their beauty, or simplicity. These then must be tested

by experiment — the surprising thing is that the aesthetic quality of a theory

is often a good predictor of that theory’s explanatory and predictive power.

This situation is summed up by William of Ockham, “Entities should not be

multiplied unnecessarily”, known as Occam’s Razor.

We start our search into an explanation of this mystery with the anthropic

principle(2). This is normally cast into either a weak form (that physical reality

must be consistent with our existence as conscious, self-aware entities) or a

strong form (that physical reality is the way it is because of our existence as

conscious, self-aware entities). The anthropic principle is remarkable in that

it generates signiﬁcant constraints on the form of the universe(2, 3). The two

main explanations for this are the Divine Creator explanation (the universe was

created deliberately by God to have properties suﬃcient to support intelligent

life), or the Ensemble explanation(3) (that there is a set, or ensemble, of diﬀerent

2

universes, diﬀering in details such as physical parameters, constants and even

laws, however, we are only aware of such universes that are consistent with our

existence). In the Ensemble explanation, the strong and weak formulations of

the anthropic principle are equivalent.

Tegmark introduces an ensemble theory based on the idea that every self-

consistent mathematical structure be accorded the ontological status of physical

existence. He then goes on to categorize mathematical structures that have

been discovered thus far (by humans), and argues that this set should be largely

universal, in that all self-aware entities should be able to uncover at least the

most basic of these mathematical structures, and that it is unlikely we have

overlooked any equally basic mathematical structures.

An alternative ensemble approach is that of Schmidhuber’s(4) — the “Great

Programmer”. This states that all possible halting programs of a universal

Turing machine have physical existence. Some of these programs’ outputs will

contain self-aware substructures — these are the programs deemed interesting

by the anthropic principle. Note that there is no need for the UTM to actually

exist, nor is there any need to specify which UTM is to be used — a program

that is meaningful on UTM1 can be executed on UTM2 by prepending it with

another program that describes UTM1 in terms of UTM2’s instructions, then

executing the individual program. Since the set of halting programs (ﬁnite

length bitstrings) is isomorphic to the set of whole numbers N, an enumeration

of N is suﬃcient to generate the ensemble that contains our universe.

In a

3

later paper(5), Schmidhuber extends his ensemble to non-halting programs, and

consider the consequences of assuming that this ensemble is generated by a

machine with bounded resources.

Each self-consistent mathematical structure (member of the Tegmark en-

semble) is completely described by a ﬁnite set of symbols, and a countable set

of axioms encoded in those symbols, and a set of rules (logic) describing how

one mathematical statement may be converted into another.1 These axioms

may be encoded as a bitstring, and the rules encoded as a program of a UTM

that enumerates all possible theorems derived from the axioms, so each mem-

ber of the Tegmark ensemble may be mapped onto a Schmidhuber one.2. The

Tegmark ensemble must be contained within the Schmidhuber one.

An alternative connection between the two ensembles is that the Schmid-

huber ensemble is a self-consistent mathematical structure, and is therefore an

element of the Tegmark one. However, all this implies is that one element of

the ensemble may in fact generate the complete ensemble again, a point made

by Schmidhuber in that the “Great Programmer” exists many times, over and

over in a recursive manner within his ensemble. This is now clearly true also of

1Strictly speaking, these systems are called recursively enumerable formal systems, and are

only a subset of the totality of mathematics, however this seem in keeping with the spirit of

Tegmark’s suggestion

2In the case of an inﬁnite number of axioms, the theorems must be enumerated using a

dovetailer algorithm. The dovetailer algorithm is a means of walking an inﬁnite level tree,

such that each level is visited in ﬁnite time. An example is that for a n-ary tree, the nodes

on the ith level are visited between steps ni and ni+1 − 1.

4

the Tegmark ensemble.

2 UNIVERSAL PRIOR

In this paper, I adopt a Schmidhuber ensemble consisting of all inﬁnite length

bitstrings, denoted {0, 1}∞. I call these inﬁnite length strings descriptions. By

contrast to Schmidhuber, I assume a uniform measure over these descriptions

— no particular string is more likely than any other. It can be shown that the

cardinality of {0, 1}∞ is the same as the cardinality of the reals, c. This set can-

not be enumerated by a dovetailer algorithm, rather the dovetailer algorithm

enumerates all ﬁnite length preﬁxes of these descriptions. Whereas in Schmid-

huber’s 1997(4) paper, the existence of the dovetailer algorithm explains the ease

with which the “Great Programmer” can generate the ensemble of universes, I

merely assume the pre-existence of all possible descriptions. The information

content of this complete set is precisely zero, as no bits are speciﬁed. It is on-

tologically equivalent to Nothing. This has been called the “zero information

principle”.

Since some of these descriptions describe self aware substructures, we can ask

the question of what these observers observe. An observer attaches sequences

of meanings to sequences of preﬁxes of one of these strings. A meaning belongs

to a countable set, which may be enumerated by the whole numbers. Thus

the act of observation may formalised as a map O : [0, 1]∞ → N. If O(x) is

5

a computable (also known as a recursive) function, then O(x) is equivalent to

a Turing machine, for which every input halts.

It is important to note that

observers must be able to evaluate O(x) within a ﬁnite amount of subjective

time, or the observer simply ceases to be. The restriction to computable O(x)

connects this viewpoint with the original viewpoint of Schmidhuber.

Another interpretation of this scenario is a state machine, possibly ﬁnite,

consuming bits of an inﬁnite length string. As each bit is consumed, the current

state of the machine is the meaning attached to the preﬁx read so far.

Under the mapping O(x), some descriptions encode for identical meanings

as other descriptions, so one should equivalence class the descriptions. In par-

ticular, strings where the bits after some bit number n are “don’t care” bits, are

in fact equivalence classes of all strings that share the ﬁrst n bits in common.

One can see that the size of the equivalence class drops oﬀ exponentially with

the amount of information encoded by the string. Under O(x), the amount of

information is not necessarily equal to the length of the string, as some of the

bits may be redundant. The sum

PO(s) = X

2−|p|,

p:O(p)=s

(1)

where |p| means the number of bits of p consumed by O in returning s, gives the

size of the equivalence class of all descriptions having meaning s This measure

distribution is known as a universal prior, or alternatively a Solomonoﬀ-Levin

distribution, in the case where O(x) is a universal preﬁx Turing machine(6).

6

The quantity

CO(x) = − log2 PO(O(x))

(2)

is a measure of the information content, or complexity of a description x.

If

only the ﬁrst n bits of the string are signiﬁcant, with no redundancy, then it

is easy to see CO(x) = n. Moreover, if O is a universal preﬁx Turing machine,

then the coding theorem(6) assures that C(x) ≈ K(x), where K(x) is the usual

Kolmogorov complexity, up to a constant independent of the length of x.

If we assume the self-sampling assumption(7, 8), essentially that we expect

to ﬁnd ourselves in one of the universes with greatest measure, subject to the

constraints of the anthropic principle. This implies we should ﬁnd ourselves in

one of the simplest (in terms of CO) possible universes capable of supporting

self-aware substructures (SASes). This is the origin of physical law — why we

live in a mathematical, as opposed to a magical universe. This is why aes-

thetic principles, and Ockam’s razor in particular are so successful at predicting

good scientiﬁc theories. This might also be called the “minimum information

principle”.

A ﬁnal comment to highlight the distinction between this approach and

Schmidhuber’s. Schmidhuber assumes that there is a given universal Turing

machine U which generates the ensemble we ﬁnd ourselves in. He even uses the

term “Great Programmer” to underscore this. Ontologically, this is no more

diﬃcult than assuming there is an ultimate theory of everything — ie a ﬁnal

set of equations from which all of physics can be derived. Occam’s razor is a

7

consequence of the resource constraints of U . In my approach, there is no given

laws or global interpreter. By considering just the resource constraints of the

observer, even in the case of the ensemble having a uniform measure, Occam’s

razor still applies.

3 THE WHITE RABBIT PARADOX

An important criticism leveled at ensemble theories is what John Leslie calls

the failure of induction(9,§4.69). If all possible universes exist, then what is to

say that our orderly, well-behaved universe won’t suddenly start to behave in

a disordered fashion, such that most inductive predictions would fail in them.

This problem has also been called the White Rabbit paradox(10), presumably

in a literary reference to Lewis Carrol.

This sort of issue is addressed by consideration of measure. We should not

worry about the universe running oﬀ the rails, provided it is extremely unlikely

to do so. Note that Leslie uses the term range to mean what we mean by

measure. At ﬁrst consideration, it would appear that there are vastly more

ways for a universe to act strangely, than for it to stay on the straight and

narrow, hence the paradox.

Evolution has taught us to be eﬃcient classiﬁers of patterns, and to be

robust in the presence of errors. It is important to know the diﬀerence between

a lion and a lion-shaped rock, and to establish that diﬀerence in real time.

8

Only a ﬁnite number of the description’s bits are processed by the classiﬁer, the

remaining being “don’t care” bits. Around each compact description is a cloud

of completely random descriptions considered equivalent by the observer. The

size of this cloud decreases exponentially with the complexity of the description.

This requirement imposes a signiﬁcant condition on O(x). Formally, each

connected component of the preimage O−1(s) must be dense, ie have nonzero

measure, in the space of descriptions.

Turing machines in general do not have this property of robustness against

errors. Single bit errors in the input typically lead to wildly diﬀerent outcomes.

However, an artiﬁcial neural network, which is a computational model inspired

by the brain does exhibit this robustness — leading to applications such as

classifying images in the presence of noisy or extraneous data.

So what are the chances of the laws of physics breaking down, and of us

ﬁnding ourselves in one of Lewis Carrol’s creations? Such a universe will have

a very complex description — for instance the coalescing of air molecules to

form a ﬁre breathing dragon would involve the complete speciﬁcation of the

states of some 1030 molecules, an absolutely stupendous amount of information,

compared with the simple speciﬁcation of the big bang and the laws of physics

that gave rise to life as we know it. The chance of this happening is equally

remote, via Eq. (1).

9

4 QUANTUM MECHANICS

In the previous sections, I demonstrate that formal mathematical systems are

the most compressible, and have highest measure amongst all members of the

Schmidhuber ensemble.

In this work, I explicitly assume the validity of the

Anthropic Principle, namely that we live in a description that is compatible

with our own existence. This is by no means a trivial assumption — it is

entirely possible that we are inhabiting a virtual reality where the laws of the

observed world needn’t be compatible with our existence. However, to date, the

Anthropic Principle has been found to be valid(2).

In order to derive consequences of the Anthropic Principle, one needs to

have a model of consciousness, or at very least some necessary properties that

conscious observer must exhibit. I will explore the consequences of just two such

properties of consciousness.

The ﬁrst assumption to be made is that observers will ﬁnd themselves em-

bedded in a temporal dimension. A Turing machine requires time to separate

the sequence of states it occupies as it performs a computation. Universal Tur-

ing machines are models of how humans compute things, so it is possible that all

conscious observers are capable of universal computation. Yet for our present

purposes, it is not necessary to assume observers are capable of universal com-

putation, merely that observers are embedded in time.

The second assumption, which is related to Marchal’s computational indeter-

minism(11), is that the simple mathematical description selected from the Schmid-

10

huber ensemble describes the evolution of an ensemble of possible experiences.

The actual world experienced by the observer is selected randomly from this

ensemble. More accurately, for each possible experience, an observer exists to

observe that possibility. Since it is impossible to distinguish between these

observers, the internal experience of that observer is as though it is chosen ran-

domly from the ensemble of possibilities. This I call the Projection Postulate.

The reason for this assumption is that it allows for very complex experiences

to be generated from a very simple process. It is a very generalised form of

Darwinian evolution, which exhibits extreme simplicity over ex nihilo creation

explanations of life on Earth. Whilst by no means certain, it does seem that

a minimum level of complexity of the experienced world is needed to support

conscious experience of that world according the the anthropic principle.

This ensemble of possibilities at time t we can denote ψ(t). Ludwig(12,D1.1)

introduces a rather similar concept of ensemble, which he equivalently calls state

to make contact with conventional terminology. At this point, nothing has been

said of the mathematical properties of ψ. I shall now endeavour to show that

ψ is indeed an element from complex Hilbert space, a fact normally assumed as

an axiom in conventional treatments of Quantum Mechanics.

The projection postulate can be modeled by a partitioning map A : ψ −→

{ψa, µa}, where a indexes the allowable range of potential observable values

corresponding to A, ψa is the subensemble satisfying outcome a and µa is the

measure associated with ψa (Pa µa = 1).

11

Finally, we assume that the generally accepted axioms of set theory and

probability theory hold. Whilst the properties of sets are well known, and

needn’t be repeated here, the Kolmogorov probability axioms are(6):

(A1) If A and B are events, then so is the intersection A ∩ B, the union A ∪ B

and the diﬀerence A − B.

(A2) The sample space S is an event, called the certain event, and the empty

set ∅ is an event, called the impossible event.

(A3) To each event E, P (E) ∈ [0, 1] denotes the probability of that event.

(A4) P (S) = 1.

(A5) If A ∩ B = ∅, then P (A ∪ B) = P (A) + P (B).

(A6) For a decreasing sequence A1 ⊃ A2 ⊃ · · · ⊃ An · · ·of events with Tn An =

∅, we have limn→∞ P (An) = 0.

Consider now the projection operator P{a} : V −→ V , acting on a ensemble

ψ ∈ V , V being the set of all such ensembles, to produce ψa = P{a}ψ, where

a ∈ S is an outcome of an observation. We have not at this stage assumed that

P{a} is linear. Deﬁne addition for two distinct outcomes a and b as follows:

P{a} + P{b} = P{a,b},

from which it follows that

PA⊂S = X
a∈A

P{a}

12

(3)

(4)

PA∪B = PA + PB − PA∩B

PA∩B = PAPB = PBPA.

(5)

(6)

These results extend to continuous sets by replacing the discrete sums by in-

tegration over the sets with uniform measure. Here, as elsewhere, we use Σ

to denote sum or integral respectively as the index variable a is discrete of

continuous.

Let the ensemble ψ ∈ V ≡ {PAψ|A ⊂ S} be a “reference state”, correspond-

ing to the certain event.

It encodes information about the whole ensemble.

Denote the probability of a set of outcomes A ⊂ S by Pψ(PAψ). Clearly

Pψ(PSψ) = Pψ(ψ) = 1

(7)

by virtue of (A4). Also, by virtue of Eq. (5) and (A4),

Pψ((PA + PB)ψ) = Pψ(PAψ) + Pψ(PBψ)

if A ∩ B = ∅.

(8)

Assume that Eq. (8) also holds for A ∩ B 6= ∅ and consider the possibility

that A and B can be identical. Eq. (8) may be written:

Pψ((aPA + bPB)ψ) = aPψ(PAψ) + bPψ(PBψ), ∀a, b ∈ N.

(9)

Thus, the set V naturally extends by means of the addition operator deﬁned by

Eq. (3) to include all linear combinations of observed states, at minimum over

the natural numbers. If A ∩ B 6= ∅, then Pψ((PA + PB)ψ) may exceed unity, so

clearly (PA + PB)ψ is not necessarily a possible observed outcome. How should

we interpret these new “nonphysical” states?

13

At each moment that an observation is possible, an observer faces a choice

about what observation to make. In the Multiverse, the observer diﬀerentiates

into multiple distinct observers, each with its own measurement basis. In this

view, there is no preferred basis(13).

The expression Pψ((aPA + bPB)ψ) must be the measure associated with

a observers choosing to partition the ensemble into {A, ¯A} and observing an

outcome in A and b observers choosing to partition the ensemble into {B, ¯B}

and seeing outcome B. The coeﬃcients a and b must be be drawn from a

measure distribution over the possible choices of measurement. The most gen-

eral measure distributions are complex, therefore the coeﬃcients, in general are

complex(14). We can comprehend easily what a positive measure means, but

what about complex measures? What does it mean to have an observer with

measure −1? It turns out that these non-positive measures correspond to ob-

servers who chose to examine observables that do not commute with our current

observable A. For example if A were the observation of an electron’s spin along

the z axis, then the states |+i + |−i and |+i − |−i give identical outcomes as far

as A is concerned. However, for another observer choosing to observe the spin

along the x axis, the two states have opposite outcomes. This is the most general

way of partitioning the Multiverse amongst observers, and we expect to observe

the most general mathematical structures compatible with our existence.

The probability function P can be used to deﬁne an inner product as follows.

Our reference state ψ can be expressed as a sum over the projected states

14

ψ = Pa∈S P{a}ψ ≡ Pa∈S ψa. Let V ∗ = L(ψa) be the linear span of this basis

set. Then, ∀φ, ξ ∈ V , such that φ = Pa∈S φaψa and ξ = Pa∈S ξaψa, the inner

product hφ, ξi is deﬁned by

hφ, ξi = X
a∈S

φ∗
aψaPψ(ψa).

It is straightforward to show that this deﬁnition has the usual properties of an

inner product, and that ψ is normalized (hψ, ψi = 1). The measures µa are

given by

(10)

(11)

µa = Pψ(ψa) = hψa, ψai

= hψ, Paψi

= |hψ, ˆψai|2,

where ˆψa = ψa/pPψ(ψa) is normalised.

Until now, we haven’t used axiom (A6). Consider a sequence of sets of

outcomes A0 ⊃ A1 . . ., and denote by A ⊂ An∀n the unique maximal subset

(possibly empty), such that ¯A Tn An = ∅. Then the diﬀerence PAi − PA is well

deﬁned, and so

h(PAi − PA)ψ, (PAi − PA)ψi = Pψ((PAi − PA)ψ)

= Pψ((PAi + P ¯A − PS)ψ)

(12)

= Pψ(PAi∩ ¯A).

By axiom (A6),

lim
n→∞

h(PAi − PA)ψ, (PAi − PA)ψi = 0,

(13)

15

so PAiψ is a Cauchy sequence that converges to PAψ ∈ V . Hence V is complete

under the inner product (10). It follows that V ∗ is complete also, and is therefore

a Hilbert space.

The most general form of evolution of ψ in continuous time is given by:

dψ
dt

= H(ψ).

(14)

Some people may think that discreteness of the world’s description (ie of the

Schmidhuber bitstring) must imply a corresponding discreteness in the dimen-

sions of the world. This is not true. Between any two points on a continuum,

there are an inﬁnite number of points that can be described by a ﬁnite string

— the set of rational numbers being an obvious, but by no means exhaustive

example. Continuous systems may be made to operate in a discrete way, elec-

tronic logic circuits being an obvious example. For the sake of connection with

conventional quantum mechanics, we will assume that time is continuous. A dis-

crete time formulation can also be derived, in which case we need a diﬀerence

equation instead of Eq. (14). Other possibilities also exist, such as the rational

numbers example mentioned before. The theory of time scales(15) could provide

a means of developing these other possibilities.

Axiom (A3) constrains the form of the evolution operator H. Since we

suppose that ψa is also a solution of Eq. 14 (ie that the act of observation does

not change the physics of the system), H must be linear. The certain event

16

must have probability of 1 at all times, so

0 =

dPψ(t)(ψ(t))
dt

= d/dthψ, ψi

= hψ, Hψi + hHψ, ψi

H† = −H,

(15)

i.e. H is i times a Hermitian operator.

5 Discussion

A conventional treatment of quantum mechanics (see eg Shankar(16)) intro-

duces a set of 4-5 postulates that appear mysterious. In this paper, I introduce

a model of observation based on the idea of selecting actual observations from an

ensemble of possible observations, and can derive the usual postulates of quan-

tum mechanics aside from the Correspondence Principle.3 Even the property

of linearity is needed to allow disjoint observations to take place simultaneously

in the universe. Weinberg(18, 19) experimented with a possible non-linear gen-

eralisation of quantum mechanics, however found great diﬃculty in producing

a theory that satisﬁed causality. This is probably due to the nonlinear terms

3The Correspondence Principle states that classical state variables are represented in the

quantum formulation by replacing appropriately x → X and p → −i~d/dx. Stenger(17)

has developed a theory based on fundamental symmetries that explains the Correspondence

Principle.

17

mixing up the partitioning {ψa, µa} over time.

It is usually supposed that

causality(3), at least to a certain level of approximation, is a requirement for a

self-aware substructure to exist. It is therefore interesting, that relatively mild

assumptions about the nature of SASes, as well as the usual interpretations of

probability and measure theory lead to a linear theory with the properties we

know of as quantum mechanics. Thus we have a reversal of the usual ontological

status between Quantum Mechanics and the Many Worlds Interpretation(20).

ACKNOWLEDGMENTS

I would like to thank the following people from the “Everything” email discus-

sion list for many varied and illuminating discussions on this and related topics:

Wei Dai, Hal Finney, Gilles Henri, James Higgo, George Levy, Alastair Malcolm,

Christopher Maloney, Jaques Mallah, Bruno Marchal and J¨urgen Schmidhuber.

In particular, the solution presented here to the White Rabbit paradox was

developed during an email exchange between myself and Alistair Malcolm during

July 1999, archived on the everything list (http://www.escribe.com/science/theory).

Alistair’s version of

this

solution may be found on his web site at

http://www.physica.freeserve.co.uk/p101.htm.

I would also like to thank the anonymous reviewer for suggesting Ludwig’s

book(12). Whilst the intuitive justiﬁcation in that book is very diﬀerent, there

is a remarkable congruence in the set of axioms chosen to the ones presented in

18

this paper.

References

[1] E. P. Wigner. Symmetries and Reﬂections (MIT Press, Cambridge, 1967).

[2] J. D. Barrow and F. J. Tipler. The Anthropic Cosmological Principle

(Clarendon, Oxford, 1986).

[3] Max Tegmark. Is ”the theory of everything” merely the ultimate ensemble

theory. Ann. Phys. 270, 1–51, (1998).

[4] J¨urgen Schmidhuber. A computer scientist’s view of life, the universe and

everything. In C. Freska, M. Jantzen, and R. Valk, editors, Foundations

of Computer Science: Potential-Theory-Cognition, volume 1337 of Lecture

Notes in Computer Science, pages 201–208 (Springer, Berlin, 1997).

[5] J¨urgen Schmidhuber. Algorithmic theories of everything. Technical Report

IDSIA-20-00, IDSIA, Galleria 2, 6928 Manno (Lugano), Switzerland, 2000.

arXiv:quant-ph/0011122.

[6] Ming Li and Paul Vit´anyi. An Introduction to Kolmogorov Complexity and

its Applications (Springer, New York, 2nd edition, 1997).

[7] J. Leslie. The End of the World (Routledge, London, 1996).

[8] B. Carter. The anthropic principle and its implications for biological evo-

lution. Phil. Trans. Roy. Soc. Lond., A310,347–363, (1983).

19

[9] John Leslie. Universes (Routledge, New York, 1989).

[10] Bruno Marchal.

Conscience

et m´ecanisme.

Technical Report

TR/IRIDIA/95, Brussels University, 1995.

[11] Bruno Marchal. Computation, consciousness and the quantum. Teorie e

modelli 6,29–44 (2001).

1983).

[12] Gunther Ludwig. Foundations of Quantum Mechanics I (Springer, Berlin,

[13] Henry P. Stapp. The basis problem in many-world theories. Canadian J.

Phys. 80,1043–1052 (2002). arXiv:quant-ph/0110148.

[14] Donald L. Cohn. Measure Theory (Birkh¨auser, Boston, 1980).

[15] Martin Bohner and Allan Peterson. Dynamic Equations on Time Scales

(Birkh¨auser, Boston, 2001).

[16] Ramamurti Shankar. Principles of Quantum Mechanics

(Plenum, New

York, 1980).

(1989).

[17] Victor Stenger.

The

comprehensible

cosmos.

Draft book:

http://spot.colorado.edu/˜vstenger/nothing.html.

[18] Steven Weinberg. Testing quantum mechanics. Ann. Phys. 194, 336–386

[19] Steven Weinberg. Dreams of a Final Theory (Pantheon, New York, 1992).

20

[20] Bryce de Witt and R. Neill Graham. The Many Worlds Interpretation of

Quantum Mechanics (Princeton UP, 1973).

21

